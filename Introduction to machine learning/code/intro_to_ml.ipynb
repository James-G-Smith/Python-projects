{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "yfjSeOkvcJv3"
   },
   "source": [
    "---\n",
    "\n",
    "# Welocme to the power of predictive analysis\n",
    "\n",
    "This notebook will walk through an example of how we use Machine Learning to provide very powerful predictions after learning the patterns in a dataset. \n",
    " \n",
    "The dataset we use in this example is a famous dataset of Iris flowers. There are three distinct species in the dataset:\n",
    "     \n",
    "     Iris Setosa\n",
    "     Iris Virginica\n",
    "     Iris Versicolor\n",
    "     \n",
    "\n",
    "![alt text](https://raw.githubusercontent.com/James-G-Smith/Python-projects/master/Introduction%20to%20ML/img/1_7bnLKsChXq94QjtAiRn40w.png)\n",
    "\n",
    "As we move along this example, keep in mind that our end goal is to be able to create a model that will take in the measurement features of a flower species' datapoint, and be able to predict, with high accuracy, which species of flower this is.\n",
    " \n",
    "We aim to make this walkthrough as followable as possible.\n",
    "\n",
    "**1. Import libraries to help with our program and access our data**\n",
    " \n",
    "There are many libraries in python that one can import into their own program. These are basically pre-written pieces of code that will work with parts of our program so that we do not have to worry about it. \n",
    " \n",
    "Think about opening a web brower on your computer by clicking its icon. There are a slew of things going on in the background, but we do not have to worry about that. Importing libraries to our program is a similar abstraction.\n",
    " \n",
    "The libraries that we import are\n",
    " \n",
    "  - _pandas (**Pan**el **Da**ta) - Analytical library for panel data. Think excel._\n",
    "  \n",
    "  - _pyplot (**Py**thon **Plot**ting) - library for visualisations._ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "G41V5oL0ZNJ-"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully imported\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "print(\"Successfully imported\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "XMHYVndrcVUx"
   },
   "source": [
    "Read the dataset into our program\n",
    "\n",
    "Now that we have imported our libraries, let's use the pandas library, and its method 'read_csv', to load in our Iris flower dataset. This method the file path of the dataset as an argument. There are many other arguments we can use but these are not important at the moment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "f-FPWUUFYnud"
   },
   "outputs": [],
   "source": [
    "iris = pd.read_csv('https://raw.githubusercontent.com/James-G-Smith/Python-projects/master/Introduction%20to%20ML/data/iris.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Ft-QNihAcei2"
   },
   "source": [
    "Observe our data\n",
    " \n",
    "It is always a good idea to check what the data looks like before carrying out any analysis. We use a few more methods below to observe this dataset. Run each cell, one by one, to see what the method does.\n",
    " \n",
    "One important thing to note here is that in the cell above, we have set **iris** equal to the pandas method that reads the data in. This means that we can now refer to our Iris Flower dataset as **iris**. We therefore call all of our methods on **iris**.\n",
    " \n",
    "Calling a method on an object in most programming languages is usually done with the following syntax:\n",
    " \n",
    "     object.method()\n",
    "     \n",
    "So in our example, calling the method **copy()** on the **iris** dataset object would be done as follows:\n",
    " \n",
    "     iris.copy()\n",
    "     \n",
    "This would create a copy of our iris dataset.\n",
    "\n",
    "Check the format of the data\n",
    " \n",
    "The **head()** method prints out the first 5 rows of our dataset. If we call this method, we can see that each row has 5 columns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "XCOUhAg2ZYD4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   sepal-length  sepal-width  petal-length  petal-width      species\n",
      "0           5.1          3.5           1.4          0.2  Iris-setosa\n",
      "1           4.9          3.0           1.4          0.2  Iris-setosa\n",
      "2           4.7          3.2           1.3          0.2  Iris-setosa\n",
      "3           4.6          3.1           1.5          0.2  Iris-setosa\n",
      "4           5.0          3.6           1.4          0.2  Iris-setosa\n"
     ]
    }
   ],
   "source": [
    "print(iris.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "I31r0NSncydG"
   },
   "source": [
    "\n",
    "\n",
    "so, as we can now see, our dataset has 4 features for each data point. These are:\n",
    " \n",
    "     sepal length\n",
    "     sepal width\n",
    "     petal length\n",
    "     petal width\n",
    "     \n",
    "![alt text](https://raw.githubusercontent.com/James-G-Smith/Python-projects/master/Introduction%20to%20ML/img/versicolor.jpg)     \n",
    "\n",
    "     \n",
    "The final column in the dataset is going to be our label, or predictor, column. This is due to the fact that we are predicting the species of the flower given the other details of the row.\n",
    " \n",
    "To remind you of our end goal, we want to create a model that will do the following\n",
    "\n",
    "![alt text](https://raw.githubusercontent.com/James-G-Smith/Python-projects/master/Introduction%20to%20ML/img/model_sketch1.png)\n",
    "\n",
    "**2. Explore our dataset**\n",
    " \n",
    "Now that we know we have the correct dataset in a format that we can work with, we can further explore our dataset to see if we can, ourselves, spot any patterns.\n",
    " \n",
    "Check the average values for each species\n",
    " \n",
    "Can we see any patterns in the dataset based on the **average** values of **each species**'?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "zOUTPzlMZc_h"
   },
   "outputs": [],
   "source": [
    "print(iris.groupby('species').mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "OadxyR65c64L"
   },
   "source": [
    "Whilst it looks like there are patterns in the measurements depending on the species, it's a lot easier to see any patterns once the data has been plotted on a graph.\n",
    " \n",
    "But before we plot data, notice that the species of the flowers are written as 'Iris-setosa'. Computers prefer numbers to work with, and when we come to our training our model, we will need to have the label column **species** as a number. So let's quickly convert our species to numbers based on the following mapping:\n",
    " \n",
    "     setosa     --> 0\n",
    "     versicolor --> 1\n",
    "     virginica  --> 2\n",
    "     \n",
    "The code is fairly involved for a new-comer, and it's not the most efficient way of doing this, but suffices for smaller datasets. Do not worry about glazing past this. All it does is the mapping we have stated directly above.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "LIWbSSwzZgaW"
   },
   "outputs": [],
   "source": [
    "for index, row in iris.iterrows():\n",
    "    if row.species == 'Iris-setosa' or row.species == 0:\n",
    "        iris.iloc[index, 4] = 0\n",
    "    elif row.species == 'Iris-versicolor' or row.species == 1:\n",
    "        iris.iloc[index, 4] = 1\n",
    "    else:\n",
    "        iris.iloc[index, 4] = 2\n",
    "\n",
    "print(iris.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "cQqx9DqEeF7x"
   },
   "source": [
    "Let's plot this data\n",
    "\n",
    "The code below splits each column and names it based on the column name."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "TrxWSt_jZkrL"
   },
   "outputs": [],
   "source": [
    "sepal_length = iris.iloc[:, 0]\n",
    "sepal_width = iris.iloc[:, 1]\n",
    "\n",
    "petal_length = iris.iloc[:, 2]\n",
    "petal_width = iris.iloc[:, 3]\n",
    "\n",
    "species = iris.iloc[:, 4]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "nOfV6X9feK_p"
   },
   "source": [
    "Relationship between sepal measurements and species?\n",
    " \n",
    "In the following plots, the colour mappings are:\n",
    " \n",
    "     purple --> setosa\n",
    "     green  --> versicolor\n",
    "     yellow --> virginica"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ija88h17ZobP"
   },
   "outputs": [],
   "source": [
    "plt.scatter(sepal_length, sepal_width, c=species)\n",
    "plt.xlabel('Sepal Length (cm)')\n",
    "plt.ylabel('Sepal Width (cm)')\n",
    "plt.title('Relationship between Sepal length and width')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ZaaibmYheS-o"
   },
   "source": [
    "It looks like there is a relatively good distinction between each species and their sepal measurements. The species represented by the purple dots, the setosa species, looks like it is completely separable from the other two species. The other two species, however, are somewhat intertwined\n",
    "\n",
    "Relationship between petal measurements and species?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "pPG-5eAgZzlf"
   },
   "outputs": [],
   "source": [
    "plt.scatter(petal_length, petal_width, c=species)\n",
    "plt.xlabel('Petal Length (cm)')\n",
    "plt.ylabel('Petal Width (cm)')\n",
    "plt.title('Relationship between Petal length and width')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "OAAQRrYReXvK"
   },
   "source": [
    "A similar conclusion can be drawn between the petal measurements and the respective species. It looks like the species represented by the purple dots, again, the setosa species, is very clearly seperable from the other species. The other two species, this time, look like they are a bit more separable. \n",
    "\n",
    "**3. Arrange our data for our model**\n",
    " \n",
    "Now that we have seen that there are patterns in the data, we are in a fantastic place to place to make predictive models. Note, however, that many real datasets do not have such clear-cut patterns, and one of the jobs of a data scientist is to actually find these patterns. This can be done by working with the original dataset or expanding it using statistical techniques.\n",
    " \n",
    "Before we forget the reason we are doing this. A brief reminderof what we want our model to do\n",
    " \n",
    "**Input features:**\n",
    "\n",
    "     Sepal Length\n",
    "     Sepal Width\n",
    "     Petal Length\n",
    "     Petal Width\n",
    "\n",
    "**Output label:**\n",
    "\n",
    "     Prediction of what species this flower is based on the input measurements\n",
    "\n",
    "We first, however, import a few more libraries that will help us in this section and the next:\n",
    "\n",
    "  - _sklearn (__S__cience kit learning) - holds numerous machine learning and other methods._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "xbk7C_BJZ5S7"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "4EZnarB9eg3D"
   },
   "source": [
    "An explanation as to what these specific imports do\n",
    " \n",
    "- train_test_split: _A method splits our whole dataset into two datasets. One for training our model and one for testing our model._\n",
    "- shuffle: _rearranges the ordering of our datasets_\n",
    "- LogisticRegression: _A class that acts as our learning model for this dataset._\n",
    "\n",
    "We first need to split our dataset into a set of data features and a label\n",
    " \n",
    "  - Features: **(sepal length, sepal width, petal length, petal width)**\n",
    "  - Label:     **(species)** \n",
    "\n",
    "\n",
    "This just rearranges the rows in the dataset so our species aren't all bunched together. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "mdoipBCiZ-AW"
   },
   "outputs": [],
   "source": [
    "iris = shuffle(iris)\n",
    "\n",
    "features = iris.iloc[:, :4]\n",
    "labels = iris.iloc[:, 4]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "wt2xMFEZbndz"
   },
   "source": [
    "So our features look like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "SFZiOSc3aCPx"
   },
   "outputs": [],
   "source": [
    "print(features.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "igr_Mo7WbiQo"
   },
   "source": [
    "and our labels look like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "FYev0BRKaGw8"
   },
   "outputs": [],
   "source": [
    "print(labels.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "zGfydqW1bWAw"
   },
   "source": [
    "Split our features and label datasets into testing and training sets\n",
    "\n",
    "We don't want to use our whole dataset to train our model, as we will then have nothing to test it on. This is why we have imported the **train_test_split** method.\n",
    "\n",
    "We keep a small percentage of examples out of the training part so that we can test and see how well our model did on classifying these datapoints. In the code below, the argument **test_size** in the **train_test_split** method means that we keep 20% of the original dataset for testing, and therefore the other 80% is used in training.\n",
    "\n",
    "We will now have datasets that look as follows. Please note that the train and test sets all correspond to eachother. So the indexes from the original dataset are kept consistent. This can be seen by the lefthand-most column when viewing these datasets. This is the index column.\n",
    "\n",
    "Training datasets (what is used to train the model):\n",
    "\n",
    "     features_train - 80% of the features dataset\n",
    "     labels_train   - 80% of the labels dataset\n",
    "     \n",
    "Testing datasets (What is used to test our model. Our model never sees these when training):\n",
    " \n",
    "     features_test  - 20% of the features dataset\n",
    "     labels_test    - 20% of the labels dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "6fCAE1ftaNEg"
   },
   "outputs": [],
   "source": [
    "features_train, features_test, labels_train, labels_test = train_test_split(features.astype(\"int\"), labels.astype(\"int\"), test_size=0.2)\n",
    "\n",
    "\n",
    "print(\"Number of data points in the training set is: \", len(labels_train))\n",
    "print(\"Number of data points in the testing set is:  \", len(labels_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "3iLQebv0bJ5B"
   },
   "source": [
    "To show to you that all of the indexes have been kept in the same place, we can print out the top of the features training dataset and the labels training dataset and observe that the index locations are identical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "tdXyfrfuaQEN"
   },
   "outputs": [],
   "source": [
    "print(features_train.head())\n",
    "print(labels_train.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "8Ep5C_pnbEaf"
   },
   "source": [
    "We can now train our model!\n",
    "\n",
    "We are now in the perfect position to enter our data into a model and have it learn any underlying patterns in the dataset. As we have already explored the dataset, we know there are some relationships and patterns, but the whole reason of machine learning is that it is lightyears ahead of humans at spotting these relationships. Imagine if the dataset had thousands of features and millions of examples. It wouldn't be so easy to plot this on a 2-dimensional graph!\n",
    "\n",
    "The model we imported above, LogisticRegression, can perform very well on classification tasks, so we use SKLearns implementation to abstract away the details. We merely want to show you the power of predictive analysis. Do not worry about the arguments to this class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Xqwal4lbaWnP"
   },
   "outputs": [],
   "source": [
    "my_model = LogisticRegression(solver='lbfgs', multi_class='ovr')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "2w_zO-0ka9sq"
   },
   "source": [
    "We have now created our model. This just sets up the initial parameters of our model. We need to call the **fit** method on this model and input our training data as arguments\n",
    "\n",
    "Note: if you have forgotten the syntax of calling methods on objects. It merely goes like this\n",
    " \n",
    "object.method(argument1, argument2, ...)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "t3fuBf_oabZF"
   },
   "outputs": [],
   "source": [
    "my_model.fit(features_train, labels_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "kNsMqNRzarhK"
   },
   "source": [
    "The call above fits our model to the dataset that we provide it. Here, that's the 80% of our feature set and corresponding 80% of labels that classify these data points. Our model trains by updating its parameters so that they provide an output of the correct label for each point in our training dataset.\n",
    "\n",
    "Test that our model predicts well\n",
    "\n",
    "The first thing we do is get the predictions of our model. We do this by calling the **predict** method of our logistic regression model, called **my_model**. This outputs the prediction for each set of features in the dataset. I.e., this will be a list of 30 label predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "eK4eFi6_ahAi"
   },
   "outputs": [],
   "source": [
    "predictions = my_model.predict(features_test)\n",
    "print(predictions)\n",
    "\n",
    "\n",
    "labels = labels_test\n",
    "print(labels.values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "AvJkCji9ajcY"
   },
   "source": [
    "It looks like we have pretty similar predictions to the actual label of the species. Let's use the **score** method to see what percentage of our predictions matches the actual class label.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "KwyI9xp0alAj"
   },
   "outputs": [],
   "source": [
    "print(my_model.score(features_test, labels_test))"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Copy of Copy of Copy of Copy of Copy of Copy of Copy of intro_to_ml.ipynb",
   "provenance": [
    {
     "file_id": "https://github.com/James-G-Smith/Python-projects/blob/master/Hackathons/Project_Hack_3.0/intro_to_ml.ipynb",
     "timestamp": 1561721939833
    },
    {
     "file_id": "https://github.com/James-G-Smith/Python-projects/blob/master/Hackathons/Project_Hack_3.0/intro_to_ml.ipynb",
     "timestamp": 1561551638575
    },
    {
     "file_id": "https://github.com/James-G-Smith/Python-projects/blob/master/Hackathons/Project_Hack_3.0/intro_to_ml.ipynb",
     "timestamp": 1561550321203
    },
    {
     "file_id": "https://github.com/James-G-Smith/Python-projects/blob/master/Hackathons/Project_Hack_3.0/intro_to_ml.ipynb",
     "timestamp": 1561549569825
    },
    {
     "file_id": "https://github.com/James-G-Smith/Python-projects/blob/master/OGTC%20-%20Mini%20Hackathon/intro_to_ml.ipynb",
     "timestamp": 1561547656975
    },
    {
     "file_id": "https://github.com/James-G-Smith/Python-projects/blob/master/OGTC%20-%20Mini%20Hackathon/intro_to_ml.ipynb",
     "timestamp": 1551971375041
    },
    {
     "file_id": "1bhaV_cfSusTSpPfWYfejRyxxxbGs67d9",
     "timestamp": 1551970992711
    }
   ],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
